{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text as sql_text, inspect\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Ensure 'perf_data' directory exists\n",
    "os.makedirs('perf_data', exist_ok=True)\n",
    "\n",
    "# Connect to the database\n",
    "db_eng = create_engine('postgresql+psycopg2://postgres:postgres@localhost:5432/airbnb',\n",
    "                       connect_args={'options': '-csearch_path={}'.format('new_york_city')},\n",
    "                       pool_size=10,  # Use connection pooling\n",
    "                       max_overflow=20,\n",
    "                       isolation_level='SERIALIZABLE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the reviews table within the same connection block\n",
    "with db_eng.connect() as conn:\n",
    "    alter_table_query = \"\"\"\n",
    "    ALTER TABLE reviews\n",
    "    ADD COLUMN IF NOT EXISTS comments_tsv tsvector;\n",
    "    \"\"\"\n",
    "    update_table_query = \"\"\"\n",
    "    UPDATE reviews\n",
    "    SET comments_tsv = to_tsvector(comments);\n",
    "    \"\"\"\n",
    "    create_index_query = \"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS comments_tsv_in_reviews\n",
    "    ON reviews USING GIN (comments_tsv);\n",
    "    \"\"\"\n",
    "    conn.execute(sql_text(alter_table_query))\n",
    "    conn.execute(sql_text(update_table_query))\n",
    "    conn.execute(sql_text(create_index_query))\n",
    "    conn.commit()\n",
    "\n",
    "    # Check if the column 'comments_tsv' exists\n",
    "    inspector = inspect(conn)\n",
    "    columns = [col['name'] for col in inspector.get_columns('reviews')]\n",
    "    if 'comments_tsv' in columns:\n",
    "        print(\"'comments_tsv' column successfully added.\")\n",
    "    else:\n",
    "        print(\"'comments_tsv' column was not added.\")\n",
    "        exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build ts query for comments_tsv\n",
    "def build_ts_query(word, date_start, date_end):\n",
    "    query = f\"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM reviews r\n",
    "    WHERE comments_tsv @@ to_tsquery('{word}')\n",
    "      AND datetime >= '{date_start}'\n",
    "      AND datetime <= '{date_end}'\n",
    "    \"\"\"\n",
    "    return query\n",
    "\n",
    "# Function to build text search query for comments\n",
    "def build_text_search_query(word, date_start, date_end):\n",
    "    query = f\"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM reviews r\n",
    "    WHERE comments ILIKE '%%{word}%%'\n",
    "      AND datetime >= '{date_start}'\n",
    "      AND datetime <= '{date_end}'\n",
    "    \"\"\"\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add or drop index and fetch current indexes on the table\n",
    "def add_drop_index(engine, action, column, table):\n",
    "    index_name = f\"idx_{column}_in_{table}\"\n",
    "    if column == 'comments_tsv':\n",
    "        index_type = 'GIN'\n",
    "    else:\n",
    "        index_type = 'BTREE'\n",
    "\n",
    "    if action == 'add':\n",
    "        query = sql_text(f\"CREATE INDEX {index_name} ON {table} USING {index_type}({column});\")\n",
    "    elif action == 'drop':\n",
    "        query = sql_text(f\"DROP INDEX IF EXISTS {index_name};\")\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate time difference\n",
    "def time_diff(start_time, end_time):\n",
    "    return (end_time - start_time).total_seconds()\n",
    "\n",
    "# Function to execute a query and measure time\n",
    "def run_query(query, conn):\n",
    "    start_time = datetime.now()\n",
    "    conn.execute(sql_text(query))\n",
    "    end_time = datetime.now()\n",
    "    return time_diff(start_time, end_time)\n",
    "\n",
    "# Function to compute performance metrics\n",
    "def compute_metrics(times):\n",
    "    return {\n",
    "        \"avg\": round(np.mean(times), 4),\n",
    "        \"min\": round(np.min(times), 4),\n",
    "        \"max\": round(np.max(times), 4),\n",
    "        \"std\": round(np.std(times), 4),\n",
    "        \"exec_count\": len(times),\n",
    "        \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "# Fetch performance data\n",
    "def fetch_perf_data(filename):\n",
    "    try:\n",
    "        with open(filename) as f:\n",
    "            if os.stat(filename).st_size == 0:\n",
    "                return {}\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "# Write performance data\n",
    "def write_perf_data(data, filename):\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create queries for each year from 2009 to 2024\n",
    "q_dict_ts = {}\n",
    "q_dict_text = {}\n",
    "years = [2009, 2010, 2011, 2012, 2013, 2014, 2017, 2019, 2023]\n",
    "words = ['apartment', 'awesome', 'horrible']\n",
    "\n",
    "for word in words:\n",
    "    for yr in years:\n",
    "        q_name_ts = f'{word}_{yr}'\n",
    "        q_name_text = f'{word}_{yr}'\n",
    "        date_start = f'{yr}-01-01 00:00:00'\n",
    "        date_end = f'{yr}-12-31 23:59:59'\n",
    "        q_dict_ts[q_name_ts] = build_ts_query(word, date_start, date_end)\n",
    "        q_dict_text[q_name_text] = build_text_search_query(word, date_start, date_end)\n",
    "\n",
    "pprint.pp(q_dict_ts)\n",
    "pprint.pp(q_dict_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_summary_path = 'perf_data/text_search_query.json'\n",
    "if not os.path.exists(perf_summary_path):\n",
    "    with open(perf_summary_path, 'w') as f:\n",
    "        json.dump({}, f)\n",
    "\n",
    "perf_summary = fetch_perf_data(perf_summary_path)\n",
    "\n",
    "# List of all indexes to test\n",
    "all_indexes = [['datetime', 'reviews'], ['comments_tsv', 'reviews']]\n",
    "\n",
    "# Different combinations of indexes to test\n",
    "specs = [\n",
    "    [],\n",
    "    [['comments_tsv', 'reviews']],\n",
    "    [['datetime', 'reviews']],\n",
    "    [['datetime', 'reviews'], ['comments_tsv', 'reviews']],\n",
    "]\n",
    "\n",
    "def process_query(query_name, query):\n",
    "    time_list = []\n",
    "    with db_eng.connect() as conn:\n",
    "        for _ in range(1):  # Run each query 10 times\n",
    "            time_list.append(run_query(query, conn))\n",
    "    perf_profile = compute_metrics(time_list)\n",
    "    return query_name, perf_profile\n",
    "\n",
    "def run_queries(query_dict):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:  # Use ThreadPoolExecutor for parallel execution\n",
    "        futures = []\n",
    "        for query_name, query in query_dict.items():\n",
    "            for spec in specs:\n",
    "                # Drop all indexes first\n",
    "                for index in all_indexes:\n",
    "                    add_drop_index(db_eng, 'drop', index[0], index[1])\n",
    "\n",
    "                # Add necessary indexes\n",
    "                for index in spec:\n",
    "                    add_drop_index(db_eng, 'add', index[0], index[1])\n",
    "\n",
    "                # Build spec key for JSON output\n",
    "                spec_key = '__' + '__'.join([f\"{index[0]}_in_{index[1]}\" for index in spec]) + '__'\n",
    "\n",
    "                futures.append(executor.submit(process_query, query_name + '__' + spec_key, query))\n",
    "        for future in futures:\n",
    "            results.append(future.result())\n",
    "    return results\n",
    "\n",
    "# Run queries for each word sequentially\n",
    "for word in ['apartment', 'awesome', 'horrible']:\n",
    "    print(f\"Running experiments for word: {word}\")\n",
    "    word_queries_ts = {k: v for k, v in q_dict_ts.items() if k.startswith(word)}\n",
    "    word_queries_text = {k: v for k, v in q_dict_text.items() if k.startswith(word)}\n",
    "\n",
    "    # Run full-text search queries with different index configurations\n",
    "    results_ts = run_queries(word_queries_ts)\n",
    "    # Run text search queries with different index configurations\n",
    "    results_text = run_queries(word_queries_text)\n",
    "\n",
    "    # Update performance summary\n",
    "    for query_name, perf_profile in results_ts + results_text:\n",
    "        base_name, spec_key = query_name.split('__', 1)\n",
    "        if base_name not in perf_summary:\n",
    "            perf_summary[base_name] = {}\n",
    "        perf_summary[base_name][spec_key] = perf_profile\n",
    "\n",
    "    write_perf_data(perf_summary, perf_summary_path)\n",
    "\n",
    "print(\"JSON files created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
