{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9d3399-cc59-4471-b192-7f45fe26f8d8",
   "metadata": {},
   "source": [
    "## <span style=color:blue>Patterns used in Programming Assignment 2 (version mostly avoiding the util.py file)  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54739ab3-0586-4f04-9365-bb0fbcbc1984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are boiler plate imports that seem useful\n",
    "# Perhaps cleaner would be to delete or comment out the ones that aren't used in this script...\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "# see https://stackoverflow.com/questions/415511/how-do-i-get-the-current-time-in-python\n",
    "#   for some basics about datetime\n",
    "\n",
    "import pprint\n",
    "\n",
    "# sqlalchemy 2.0 documentation: https://www.sqlalchemy.org/\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text as sql_text\n",
    "\n",
    "# the following is deprecated, it seems, so using the sqlalchemy\n",
    "# from pyscopg2 import sqlio\n",
    "\n",
    "# the file in benchmarking/util.py should hold utilities useful for your benchmarking exercise\n",
    "# In this notebook we have commented out all mentions of util, so that you can run\n",
    "#    this notebook before setting up your benchmarking/util.py file\n",
    "# sys.path.append('benchmarking/')\n",
    "import util\n",
    "# to invoke a function \"foo()\" inside util.py, use \"util.foo()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0392a9-7e72-4021-99d0-3f84a1ba4ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "# test that utils.py has been imported well\n",
    "util.hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681f277-6dfe-4262-b50c-56eb63653748",
   "metadata": {},
   "source": [
    "### <span style=color:blue>For this exercise you will use four .csv files from AirBnB.</span>\n",
    "\n",
    "<span style=color:blue>You can find the files at https://drive.google.com/drive/folders/14gWh0ck3vzWxyakaWHHH38AgWY7UC-IQ?usp=sharing </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a4b6c-563f-425d-bfa6-bf162f8b4bb7",
   "metadata": {},
   "source": [
    "### <span style=color:blue>Setting up Postgres connection.  Note database name is \"airbnb\" </span>\n",
    "\n",
    "### <span style=color:blue>Note: this should be modified so that the user name/password are not included into the program. </span>\n",
    "\n",
    "<span style=color:blue>E.g., see https://docs.sqlalchemy.org/en/20/core/engines.html for how to construct the URLs that the create_engine command uses.  Also, one should store the user/password into environment variables and read them in to populate the URL.  </span>\n",
    "\n",
    "<span style=color:blue>E.g., see https://stackoverflow.com/questions/4906977/how-can-i-access-environment-variables-in-python for how to work with environment variables on mac, </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c4fe37-23c6-4b4f-9554-26aec6a7b0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created db engine.\n"
     ]
    }
   ],
   "source": [
    "# following https://www.geeksforgeeks.org/connecting-postgresql-with-sqlalchemy-in-python/\n",
    "\n",
    "db_eng = create_engine('postgresql+psycopg2://postgres:postgres@localhost:5432/airbnb',\n",
    "                       connect_args={'options': '-csearch_path={}'.format('new_york_city')},\n",
    "                       isolation_level = 'SERIALIZABLE')\n",
    "#    , echo=True)\n",
    "#    , echo_pool=\"debug\")\n",
    "\n",
    "print(\"Successfully created db engine.\")\n",
    "\n",
    "# connect_args is used to set search_path to the schema 'new_york_city' in the airbnb database\n",
    "\n",
    "# isolation_level SERIALIZABLE makes transactions happen in sequence, which is good \n",
    "#      for the benchmarking we will be doing\n",
    "\n",
    "# for general info on sqlalchemy connections,\n",
    "#    see: https://docs.sqlalchemy.org/en/20/core/connections.html\n",
    "\n",
    "# echo from https://docs.sqlalchemy.org/en/20/core/engines.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bcf3c-b5d3-40c3-ad4d-6c2df3f27215",
   "metadata": {},
   "source": [
    "### <span style=color:blue>Here is a pattern for using db_eng for queries</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c2fb46-6aa3-4905-a056-84612a42b118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'sqlalchemy.engine.cursor.CursorResult'>\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "[('2595', '28794060', datetime.date(2015, 3, 30), '27436102', 'Kellie', 'Jennifer was very good at communicating with us prior to our arrival. Although she could not meet with us herself, she had a friend meet with us to g ... (537 characters truncated) ... or, so we definitely got our workout in going up and down those stairs each day! \\r<br/>\\r<br/>We had a blast! \\r<br/>\\r<br/>Thanks Jennifer! \\r<br/>'),\n",
      " ('2595', '30430122', datetime.date(2015, 4, 21), '6429364', 'Sonya', 'I love this space.  It is truly a gem in the heart of Manhattan.  The location could not be better.  Very clean, comfy, and great warm inviting energy.  Jennifer was very clear and responsive.'),\n",
      " ('2595', '32532759', datetime.date(2015, 5, 19), '12146524', 'Michiel', 'This was our first Airbnb experience, and Jennifer really made it great! No more hotels for us in the future ;) The communication was friendly and she replied very fast. Just great!!')]\n",
      "\n",
      "  listing_id        id        date reviewer_id reviewer_name  \\\n",
      "0       2595  28794060  2015-03-30    27436102        Kellie   \n",
      "1       2595  30430122  2015-04-21     6429364         Sonya   \n",
      "2       2595  32532759  2015-05-19    12146524       Michiel   \n",
      "\n",
      "                                            comments  \n",
      "0  Jennifer was very good at communicating with u...  \n",
      "1  I love this space.  It is truly a gem in the h...  \n",
      "2  This was our first Airbnb experience, and Jenn...  \n",
      "\n",
      "[(28465,)]\n",
      "\n",
      "   count\n",
      "0  28465\n"
     ]
    }
   ],
   "source": [
    "q1 = \"\"\" \n",
    "SELECT *\n",
    "FROM reviews \n",
    "WHERE date >= '2015-01-01' \n",
    "  AND date <= '2015-12-31' \n",
    "\"\"\"\n",
    "q2 = \"\"\" \n",
    "SELECT count(*)\n",
    "FROM reviews \n",
    "WHERE date >= '2015-01-01' \n",
    "  AND date <= '2015-12-31' \n",
    "\"\"\"\n",
    "\n",
    "# You can use conn.execute, which populates a cursor, in this case \"result1\" or \"result2\"\n",
    "# Alternatively, you can use pd.read_sql, which populates a dataframe\n",
    "with db_eng.connect() as conn:\n",
    "    result1 = conn.execute(sql_text(q1))   # sql_text was part of import from psycopg2\n",
    "    df1 = pd.read_sql(q1, con=conn)\n",
    "    \n",
    "    result2 = conn.execute(sql_text(q2))\n",
    "    df2 = pd.read_sql(q2, con=conn)\n",
    "    # conn.close() is automatically added to the end of this block\n",
    "\n",
    "print()\n",
    "print(type(result1))\n",
    "print()\n",
    "print(type(df1))\n",
    "print()\n",
    "pprint.pp(result1.fetchmany(3), width=120)\n",
    "print()\n",
    "pprint.pp(df1.head(3))\n",
    "print()\n",
    "print(result2.all())            # result is small, so can fetch all of it\n",
    "print()\n",
    "pprint.pp(df2.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f4784-d42c-48cd-8882-67f9a481fbfd",
   "metadata": {},
   "source": [
    "### <span style=color:blue>Example of pattern for creating parameterized functions for creating (parameterized) queries</span>\n",
    "\n",
    "<span style=color:blue>As part of Programming Assignment 2, you will create several query building functions,\n",
    "and put them into your utils.py file</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a06d531-3d80-4508-b266-1ead81bd1633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT count(*)\n",
      "FROM reviews\n",
      "WHERE date >= '2015-01-01'\n",
      "  AND date <= '2015-12-31';\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_query_reviews_count(date1, date2):\n",
    "    q21 = \"\"\"\n",
    "SELECT count(*)\n",
    "FROM reviews\n",
    "WHERE date >= '\"\"\"\n",
    "    q22 = \"\"\"'\n",
    "  AND date <= '\"\"\"\n",
    "    q23 = \"\"\"';\n",
    "\"\"\"\n",
    "    return q21 + date1 + q22 + date2 + q23\n",
    "\n",
    "print(build_query_reviews_count('2015-01-01', '2015-12-31'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def1384-4d12-4e29-9a6c-1d7a59a730b0",
   "metadata": {},
   "source": [
    "<span style=color:blue>We now show a query that will be used below to illustrated various things. You should build a function, perhaps called \"build_query_listings_join_reviews\" that takes two parameters for start date and end date, that can build this kind of query. </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a80ee9f7-ec02-458b-a7ea-60c5a7600d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q_listings_join_reviews_2015 = \"\"\"\n",
    "SELECT DISTINCT l.id, l.name\n",
    "FROM listings l, reviews r \n",
    "WHERE l.id = r.listing_id\n",
    "  AND r.date >= '2015-01-01'\n",
    "  AND r.date <= '2015-12-31'\n",
    "ORDER BY l.id;\n",
    "\"\"\"\n",
    "\n",
    "# The following code would work if you have the function build_query_listings_join_reviews()\n",
    "#    defined in your util.py file\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "q_dict = {}\n",
    "\n",
    "q_dict['listings_join_reviews_2013'] = util.build_query_listings_join_reviews('2013-01-01', '2013-12-31')\n",
    "# note: The reviews table has 7,317 entries in 2013\n",
    "\n",
    "q_dict['listings_join_reviews_2015'] = util.build_query_listings_join_reviews('2015-01-01', '2015-12-31')\n",
    "# note: The reviews table has 28,465 entries in 2015\n",
    "\n",
    "q_dict['listings_join_reviews_2019'] = util.build_query_listings_join_reviews('2019-01-01', '2019-12-31')\n",
    "# note: The reviews table has 126,469 entries in 2019\n",
    "\n",
    "q_dict['listings_join_reviews_2023'] = util.build_query_listings_join_reviews('2023-01-01', '2023-12-31')\n",
    "# note: The reviews table has 228,831 entries in 2023\n",
    "\n",
    "print(q_dict['listings_join_reviews_2013'])\n",
    "print()\n",
    "print(q_dict['listings_join_reviews_2015'])\n",
    "print()\n",
    "print(q_dict['listings_join_reviews_2019'])\n",
    "print()\n",
    "print(q_dict['listings_join_reviews_2023'])\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5addbb-f68e-409b-ae76-e0695ae36f39",
   "metadata": {},
   "source": [
    "### <span style=color:blue>Here is a pattern for computing the run-time of something, e.g., a query or an update.</span>\n",
    "\n",
    "<span style=color:blue>You should also put this into your util.py file.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a13158-d215-47ed-8582-d90af6b62794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.505509\n"
     ]
    }
   ],
   "source": [
    "def time_diff(time1, time2):\n",
    "    return (time2-time1).total_seconds()\n",
    "\n",
    "# testing it:\n",
    "time1 = datetime.now()\n",
    "# put query or update code in place of sleep command\n",
    "time.sleep(0.5)\n",
    "time2 = datetime.now()\n",
    "\n",
    "print(time_diff(time1,time2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce092baf-c301-4e00-b865-a7c563bcc4f0",
   "metadata": {},
   "source": [
    "### <span style=color:blue>Here is an example of running a query multiple times, and keeping track of run times</span>\n",
    "\n",
    "<span style=color:blue>As part of Programming Assignment 2, you should create a general-purpose function for doing this,\n",
    "and put it into your utils.py file<span>\n",
    "\n",
    "<span style=color:blue>In the illustration below we read the output of the query into a dataframe, which ensures that the entire output is computed and exported by PostgreSQL.  If we read the output into a cursor, then PostgreSQL might use a \"lazy\" approach, and not compute the full query output until we scroll through the cursor. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49632f1f-a1df-4725-bf23-177fe9168b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.071657,\n",
      " 0.042683,\n",
      " 0.040902,\n",
      " 0.025068,\n",
      " 0.023086,\n",
      " 0.022078,\n",
      " 0.022635,\n",
      " 0.021678,\n",
      " 0.021392,\n",
      " 0.021694,\n",
      " 0.022289,\n",
      " 0.02242,\n",
      " 0.022238,\n",
      " 0.022176,\n",
      " 0.020922,\n",
      " 0.021789,\n",
      " 0.056477,\n",
      " 0.024487,\n",
      " 0.021513,\n",
      " 0.021498]\n",
      "0.0284 0.0209 0.0717 0.0135\n"
     ]
    }
   ],
   "source": [
    "# we will use the query q_listings_join_reviews_2015 defined above\n",
    "\n",
    "count = 20\n",
    "\n",
    "time_list = []\n",
    "for i in range(0,count):\n",
    "    time_start = datetime.now()\n",
    "    # Open new db connection for each execution of the query to avoid multithreading\n",
    "    with db_eng.connect() as conn:\n",
    "        df = pd.read_sql(q_listings_join_reviews_2015, con=conn)\n",
    "\n",
    "    time_end = datetime.now()\n",
    "    diff = time_diff(time_start, time_end)\n",
    "    time_list.append(diff)\n",
    "\n",
    "pprint.pp(time_list)\n",
    "print(round(sum(time_list)/len(time_list), 4), \\\n",
    "        round(min(time_list), 4), \\\n",
    "        round(max(time_list), 4), \\\n",
    "        round(np.std(time_list), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d58fd0e-1fef-4679-982d-11a641c20708",
   "metadata": {},
   "source": [
    "### <span style=color:blue>Here is a pattern for adding/dropping indexes. </span>\n",
    "\n",
    "<span style=color:blue>As part of programming exercise 2 you should create a general-purpose parameterized function that can be used to add or drop an index with a given name, focused on a given table, and on a given column of that table.  After testing that the function behaves as you expect it then you should put that function into the file utils.py. </span>\n",
    "\n",
    "<span style=color:blue>For this function, I used the name add_drop_index() with four arguments:  db_eng, add/drop, column to index, table.  I assume a systematic naming of the indexes, having the form <col-name>_in_<table_name></span>\n",
    "\n",
    "<span style=color:blue>(The \"show_indexes\" queries are mainly for testing that the add/drop index functions are working correctly.)<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95fc122-63ea-4cec-9ace-6e5da5d970e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The set of indexes on reviews is: \n",
      "[('new_york_city', 'reviews', 'date_in_reviews', None, 'CREATE INDEX date_in_reviews ON new_york_city.reviews USING btree (date)')]\n"
     ]
    }
   ],
   "source": [
    "q_create_date_in_reviews = '''\n",
    "BEGIN TRANSACTION;\n",
    "CREATE INDEX IF NOT EXISTS date_in_reviews\n",
    "ON reviews(date);\n",
    "END TRANSACTION;\n",
    "'''\n",
    "\n",
    "q_drop_date_in_reviews = '''\n",
    "BEGIN TRANSACTION;\n",
    "DROP INDEX IF EXISTS date_in_reviews;\n",
    "END TRANSACTION;\n",
    "'''\n",
    "\n",
    "q_show_indexes_for_reviews = '''\n",
    "select *\n",
    "from pg_indexes\n",
    "where tablename = 'reviews';\n",
    "'''\n",
    "\n",
    "# by using a code block, it ensures that after completion \n",
    "#     the change to the indexes will be committed in the database\n",
    "with db_eng.connect() as conn:\n",
    "    conn.execute(sql_text(q_create_date_in_reviews))\n",
    "    # conn.execute(sql_text(q_drop_date_in_reviews))\n",
    "    result_reviews = conn.execute(sql_text(q_show_indexes_for_reviews))\n",
    "    print()\n",
    "    print('The set of indexes on reviews is: ')\n",
    "    print(result_reviews.all())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd73ff7-6bd0-4bce-a95c-1f3a50b15fa2",
   "metadata": {},
   "source": [
    "### <span style=color:blue>Now there is an index on the date column of reviews.  Rerun the preceding cell to see if the performance on the query q_listings_join_reviews_2015 has changed </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7121a925-35ad-44d6-ab9d-9cd8fb9afc26",
   "metadata": {},
   "source": [
    "### <span style=color:blue>The performance results will be held in a file 'perf_data/perf_summary.json' in your base directory. The format of this json file is described here. </span>\n",
    "\n",
    "<span style=color:blue> Also, this cell shows functions for fetching the previous performance data (stored as json in  \"perf_data/perf_summary.json\"), and then writing it out again (after you have adding in more data).  This will allow you to run numerous tests at different times, but keep all of the results in one place.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6260fd83-359d-4dc7-9c64-e1d289fa5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the key for each entry of perf_dict will be the name of a query or update\n",
    "# the value for each entry of perf_dict will be a \"perf_dict\" with keys that \n",
    "#     list all indexes that were in force at the time of the test run.  E.g.:\n",
    "# \n",
    "#        { '__' : ...,                                     -- i.e., no indexes in force\n",
    "#          '__id_in_listings__' : ...,                     -- indexes in force: { id_in_listings }  \n",
    "#          '__date_in_reviews__' : ...,                    -- indexes in force: { date_in_reviews }\n",
    "#          '__date_in_reviews__id_in_listings__' : ... }   -- indexes in force: { date_in_reviews, id_in_listings }\n",
    "\n",
    "# the value for each entry of the inner dict will have be a \"performance profile\" (perf_prof):\n",
    "#       having shape {avg: ..., min: ..., max: ..., std: ...}\n",
    "# (please see below for an example)\n",
    "\n",
    "\n",
    "# fetches filename (which should be a json file) and returns a \n",
    "#       dict corresponding to the contents of filename\n",
    "def fetch_perf_data(filename):\n",
    "    f = open('perf_data/' + filename)\n",
    "    return json.load(f)\n",
    "\n",
    "# writes the dictionary in dict as a json file into filename\n",
    "def write_perf_data(dict, filename):\n",
    "    with open('perf_data/' + filename, 'w') as fp:\n",
    "        json.dump(dict, fp)\n",
    "\n",
    "# testing:\n",
    "# test = { 'foo': 'goo', 'foo1' : {'hoo': 'boo', 'zoo': 'loo'}}\n",
    "# write_perf_data(test, 'test.json')\n",
    "# dict = fetch_perf_data('test.json')\n",
    "# pprint.pp(dict, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea31d6e-fae5-47a8-bad8-f0fec2a4fbad",
   "metadata": {},
   "source": [
    "<span style=color:blue>Run the next code once to initialize the file 'perf_data/perf_summary.json'; then comment it out!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "181c60a2-db8a-4c20-be9a-2758422495e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the performance data perf_summary.json file to {}\n",
    "write_perf_data({}, 'perf_summary.json')\n",
    "\n",
    "# sanity check\n",
    "# perf_summary = fetch_perf_data('perf_summary.json')\n",
    "# pprint.pp(perf_summary, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5470842-cb40-4fd0-b357-0eb974aafc99",
   "metadata": {},
   "source": [
    "### <span style=color:blue>Here is an illustration of how you can perform one test (with specified indexes) on one query</span>\n",
    "\n",
    "#### <span style=color:blue>CAUTION: the next cell is using two functions that I have set up in my benchmarking/util.py file, so it will not run for you until you set up these functions.  </span>\n",
    "\n",
    "<span style=color:blue>As part of the progamming exercise, you should create one or more parameterized functions that will enable you to invoke this kind of test numerous times, on a selected query/update and a set of selected indexes.\n",
    "\n",
    "<span style=color:blue>To provide a small illustration of the family of performance values that you will be obtaining I have run the following cell four times on the same query, but using different combinations of indexes.  Can you explain why there are different running times for different combinations of indexes?  Also, do you get roughly the same numbers as I do -- why or why not?  Do you get the same numbers if you run the test for a given set of indexes twice -- why or why not?</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5faf9d46-7de7-468a-a1f4-0da9dbd88645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spec:  [['id', 'listings'], ['date', 'reviews']] \n",
      "\n",
      "\n",
      "After doing the drop for ['date', 'calendar'] the indexes on table \"calendar\" are: \n",
      "[]\n",
      "\n",
      "After doing the add for ['id', 'listings'] the indexes on table \"listings\" are: \n",
      "[]\n",
      "\n",
      "After doing the add for ['date', 'reviews'] the indexes on table \"reviews\" are: \n",
      "[{'name': 'date_in_reviews', 'unique': False, 'column_names': ['date'], 'include_columns': [], 'dialect_options': {'postgresql_include': []}}]\n",
      "\n",
      "The list of running times is as follows:\n",
      "[0.029336,\n",
      " 0.026117,\n",
      " 0.022613,\n",
      " 0.024591,\n",
      " 0.022301,\n",
      " 0.022839,\n",
      " 0.023922,\n",
      " 0.024567,\n",
      " 0.025123,\n",
      " 0.023189,\n",
      " 0.024195,\n",
      " 0.023187,\n",
      " 0.022572,\n",
      " 0.02288,\n",
      " 0.02356,\n",
      " 0.036252,\n",
      " 0.025617,\n",
      " 0.025638,\n",
      " 0.03747,\n",
      " 0.024625,\n",
      " 0.022806,\n",
      " 0.024367,\n",
      " 0.024131,\n",
      " 0.035865,\n",
      " 0.020534,\n",
      " 0.022789,\n",
      " 0.023405,\n",
      " 0.02227,\n",
      " 0.020933,\n",
      " 0.023327,\n",
      " 0.02182,\n",
      " 0.057229,\n",
      " 0.024411,\n",
      " 0.021928,\n",
      " 0.021417,\n",
      " 0.023558,\n",
      " 0.022256,\n",
      " 0.022575,\n",
      " 0.023098,\n",
      " 0.022832,\n",
      " 0.022907,\n",
      " 0.021828,\n",
      " 0.022255,\n",
      " 0.023466,\n",
      " 0.022171,\n",
      " 0.022542,\n",
      " 0.023991,\n",
      " 0.021139,\n",
      " 0.024124,\n",
      " 0.024035]\n",
      "\n",
      "The statistics on the list of running times are as follows:\n",
      "{'avg': 0.0248, 'min': 0.0205, 'max': 0.0572, 'std': 0.0058}\n",
      "\n",
      "The new value for\"__date_in_reviews__id_in_listings__\"will be {'avg': 0.0248, 'min': 0.0205, 'max': 0.0572, 'std': 0.0058}\n",
      "\n",
      "Before modifying perf_dict, the value of perf_summary[query_name] had empty value\n",
      "\n",
      "\n",
      "After modifying perf_dict, the value of perf_summary[query_name] is: \n",
      "{'__date_in_reviews__id_in_listings__': {'avg': 0.0248,\n",
      "                                         'min': 0.0205,\n",
      "                                         'max': 0.0572,\n",
      "                                         'std': 0.0058}}\n",
      "\n",
      "\n",
      "The full value of perf_summary is:\n",
      "{'listings_join_reviews_2015': {'__date_in_reviews__id_in_listings__': {'avg': 0.0248,\n",
      "                                                                        'min': 0.0205,\n",
      "                                                                        'max': 0.0572,\n",
      "                                                                        'std': 0.0058}}}\n"
     ]
    }
   ],
   "source": [
    "# the variable all_indexes will hold all of the indexes involved in your testing.\n",
    "#   For now there are 3 indexes, but there will be more.  set of all indexes will get bigger once we do more explorations\n",
    "# Here, a pair ['col','table'] refers to an index on column 'col' in table 'table'\n",
    "# (in an ideal world, we would keep a copy of this on disk, probably in your computer's file system,\n",
    "#   and read it in when we want to use it and/or add to it.  For the full Programming Assignment 2\n",
    "#   we will be working with 4 to 6 indexes)\n",
    "\n",
    "all_indexes = [['date','reviews'], ['date','calendar'], ['id','listings']] \n",
    "\n",
    "\n",
    "# pull in performance summary from previous tests done\n",
    "perf_summary = fetch_perf_data('perf_summary.json')\n",
    "\n",
    "# we will use the same query as above, and call it 'listings_join_reviews_2015'\n",
    "#   in perf_summary.json, info about different runs for this query are\n",
    "#   held in perf_summary[<<query_name>>]\n",
    "\n",
    "# q = q_dict[query_name]\n",
    "q_listings_join_reviews_2015 = \"\"\"\n",
    "SELECT DISTINCT l.id, l.name\n",
    "FROM listings l, reviews r \n",
    "WHERE l.id = r.listing_id\n",
    "  AND r.date >= '2015-01-01'\n",
    "  AND r.date <= '2015-12-31'\n",
    "ORDER BY l.id;\n",
    "\"\"\"\n",
    "\n",
    "query_name = 'listings_join_reviews_2015'\n",
    "\n",
    "\n",
    "# here the spec is a listing of column-table pairs corresponding to indexes that are\n",
    "#    to be included in the test\n",
    "# I have run this jupyter cell on the 4 specs listed below\n",
    "spec = [['id','listings'], ['date','reviews']]\n",
    "# spec = [['date','reviews']]\n",
    "# spec = [['id','listings']]\n",
    "# spec = []\n",
    "\n",
    "# count will hold the number of times we want to run the query\n",
    "count = 50\n",
    "\n",
    "print('Processing spec: ', str(spec), '\\n')\n",
    "for index in all_indexes:\n",
    "    if index not in spec:\n",
    "        mod_index = util.add_drop_index(db_eng, 'drop', index[0], index[1])\n",
    "        print('\\nAfter doing the drop for', str(index), 'the indexes on table \"' + index[1] + '\" are: ')\n",
    "        print(mod_index)\n",
    "        \n",
    "for index in spec:\n",
    "    mod_index = util.add_drop_index(db_eng, 'add', index[0], index[1])\n",
    "    print('\\nAfter doing the add for', str(index), 'the indexes on table \"' + index[1] + '\" are: ')\n",
    "    print(mod_index)\n",
    "\n",
    "time_list = []\n",
    "for i in range(0,count):\n",
    "    time_start = datetime.now()\n",
    "    # Open new db connection for each execution of the query to avoid multithreading\n",
    "    with db_eng.connect() as conn:\n",
    "        df = pd.read_sql(q_listings_join_reviews_2015, con=conn)\n",
    "    time_end = datetime.now()\n",
    "    diff = time_diff(time_start, time_end)\n",
    "    time_list.append(diff)\n",
    "    \n",
    "perf_profile = {}\n",
    "perf_profile['avg'] = round(sum(time_list)/len(time_list), 4)\n",
    "perf_profile['min'] = round(min(time_list), 4)\n",
    "perf_profile['max'] = round(max(time_list), 4)\n",
    "perf_profile['std'] = round(np.std(time_list), 4)\n",
    "\n",
    "print('\\nThe list of running times is as follows:')\n",
    "pprint.pp(time_list)\n",
    "\n",
    "print('\\nThe statistics on the list of running times are as follows:')\n",
    "pprint.pp(perf_profile)\n",
    "\n",
    "# util.build_index_description_key() creates a listing of strings corresponding\n",
    "#    to the entries in spec, and concatenates them in the ordering given by all_indexes\n",
    "#    For example, the description_key associated with having indexes date_in_reviews and id_in_listings\n",
    "#        would be __date_in_reviews__id_in_listings__'\n",
    "#        (You probably want to use a uniform ordering of index names when you create these description_keys\n",
    "key_value = util.build_index_description_key(all_indexes, spec)\n",
    "print('\\nThe new value for\"' + key_value + '\"will be', str(perf_profile))\n",
    "\n",
    "\n",
    "# we may have run some other tests with the query q_listings_join_reviews_2015' and\n",
    "#   we don't want to overwrite those.  So we need to get the full contents\n",
    "#   of perf_summary['listings_join_reviews_2015'] and then\n",
    "#   write (or overwrite) the value for the current list of indexes used\n",
    "\n",
    "if query_name in perf_summary:\n",
    "    perf_dict = perf_summary[query_name]\n",
    "    print(\"\\nBefore modifying perf_dict, the value of perf_summary[query_name] (if it existed) was: \")\n",
    "    pprint.pp(perf_dict)\n",
    "else:\n",
    "    perf_dict = {}\n",
    "    print(\"\\nBefore modifying perf_dict, the value of perf_summary[query_name] had empty value\")\n",
    "print()\n",
    "perf_dict[key_value] = perf_profile\n",
    "perf_summary['listings_join_reviews_2015'] = perf_dict\n",
    "\n",
    "print(\"\\nAfter modifying perf_dict, the value of perf_summary[query_name] is: \")\n",
    "pprint.pp(perf_summary[query_name])\n",
    "print()\n",
    "\n",
    "print('\\nThe full value of perf_summary is:')\n",
    "pprint.pp(perf_summary)\n",
    "\n",
    "write_perf_data(perf_summary, 'perf_summary.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb4335-dd2c-4413-a539-bcc3409fc4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2bf24-6089-4069-85c8-c93fbd65ae80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
